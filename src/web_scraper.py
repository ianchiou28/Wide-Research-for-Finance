import requests
from bs4 import BeautifulSoup
from datetime import datetime
from typing import List, Dict, Optional
import urllib.parse
import os

class WebScraper:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        # å¾®åšéœ€è¦Cookieæ‰èƒ½è·å–åˆ°å®æ—¶çƒ­æœ
        self.weibo_headers = self.headers.copy()
        self.weibo_headers['Cookie'] = os.getenv('WEIBO_COOKIE', '')
    
    def scrape_fed(self) -> List[Dict]:
        """ç¾è”å‚¨æ–°é—»"""
        try:
            r = requests.get('https://www.federalreserve.gov/newsevents.htm', headers=self.headers, timeout=10)
            soup = BeautifulSoup(r.text, 'html.parser')
            articles = []
            for item in soup.select('.row.eventlist__event')[:3]:
                title = item.select_one('.eventlist__event__title')
                if title:
                    articles.append({
                        'title': title.get_text(strip=True),
                        'content': item.get_text(strip=True)[:500],
                        'source': 'ç¾è”å‚¨',
                        'category': 'us_official',
                        'url': 'https://www.federalreserve.gov' + title.find('a')['href'] if title.find('a') else '',
                        'published_at': datetime.now().isoformat()
                    })
            return articles
        except:
            return []
    
    def scrape_sec(self) -> List[Dict]:
        """SECå…¬å‘Š"""
        try:
            r = requests.get('https://www.sec.gov/news/pressreleases', headers=self.headers, timeout=10)
            soup = BeautifulSoup(r.text, 'html.parser')
            articles = []
            for item in soup.select('.views-row')[:3]:
                title = item.select_one('.views-field-title a')
                if title:
                    articles.append({
                        'title': title.get_text(strip=True),
                        'content': item.get_text(strip=True)[:500],
                        'source': 'SECå…¬å‘Š',
                        'category': 'us_official',
                        'url': 'https://www.sec.gov' + title['href'],
                        'published_at': datetime.now().isoformat()
                    })
            return articles
        except:
            return []
    
    def scrape_pbc(self) -> List[Dict]:
        """ä¸­å›½äººæ°‘é“¶è¡Œ"""
        try:
            r = requests.get('http://www.pbc.gov.cn/goutongjiaoliu/113456/113469/index.html', timeout=10)
            r.encoding = 'utf-8'
            soup = BeautifulSoup(r.text, 'html.parser')
            articles = []
            for item in soup.select('ul.list li')[:3]:
                link = item.find('a')
                if link:
                    articles.append({
                        'title': link.get_text(strip=True),
                        'content': link.get_text(strip=True),
                        'source': 'ä¸­å›½äººæ°‘é“¶è¡Œ',
                        'category': 'china_official',
                        'url': 'http://www.pbc.gov.cn' + link['href'],
                        'published_at': datetime.now().isoformat()
                    })
            return articles
        except:
            return []
    
    def scrape_csrc(self) -> List[Dict]:
        """è¯ç›‘ä¼š"""
        try:
            r = requests.get('https://www.csrc.gov.cn/csrc/c100028/common_list.shtml', timeout=10)
            r.encoding = 'utf-8'
            soup = BeautifulSoup(r.text, 'html.parser')
            articles = []
            for item in soup.select('.fl_list li')[:3]:
                link = item.find('a')
                if link:
                    articles.append({
                        'title': link.get_text(strip=True),
                        'content': link.get_text(strip=True),
                        'source': 'è¯ç›‘ä¼š',
                        'category': 'china_official',
                        'url': 'https://www.csrc.gov.cn' + link.get('href', ''),
                        'published_at': datetime.now().isoformat()
                    })
            return articles
        except:
            return []
    
    def scrape_weibo_hot_search(self) -> List[Dict]:
        """å¾®åšè´¢ç»çƒ­æœ"""
        articles = []
        try:
            # å¾®åšçƒ­æœæ¦œçš„ç§»åŠ¨ç«¯é¡µé¢ç›¸å¯¹ç®€å•
            r = requests.get('https://m.weibo.cn/api/container/getIndex?containerid=106003type%3D25%26t%3D3%26disable_hot%3D1%26filter_type%3Drealtimehot', headers=self.weibo_headers, timeout=10)
            data = r.json()
            for item in data['data']['cards'][0]['card_group'][:5]: # å–å‰5æ¡
                title = item['desc']
                articles.append({
                    'title': f"å¾®åšçƒ­æœ: {title}",
                    'content': item.get('desc_extr', title),
                    'source': 'å¾®åšçƒ­æœ',
                    'category': 'hot_search',
                    'url': item.get('scheme', ''),
                    'published_at': datetime.now().isoformat()
                })
            return articles
        except Exception as e:
            print(f"  âš  å¾®åšçƒ­æœ: æ— æ³•è·å–ï¼Œå¯èƒ½éœ€è¦æœ‰æ•ˆçš„Cookie. Error: {e}")
            return []

    def scrape_tonghuashun(self) -> List[Dict]:
        """åŒèŠ±é¡º7x24å¿«è®¯"""
        articles = []
        try:
            r = requests.get('http://news.10jqka.com.cn/realtimenews.html', headers=self.headers, timeout=10)
            r.encoding = 'gbk'
            soup = BeautifulSoup(r.text, 'html.parser')
            for item in soup.select('.main-fl ul.list-con li')[:5]: # å–å‰5æ¡
                title_tag = item.select_one('a')
                if title_tag:
                    title = title_tag.get_text(strip=True)
                    content = item.select_one('span').get_text(strip=True) if item.select_one('span') else title
                    articles.append({
                        'title': title,
                        'content': content,
                        'source': 'åŒèŠ±é¡º',
                        'category': 'china_news',
                        'url': title_tag['href'],
                        'published_at': datetime.now().isoformat()
                    })
            return articles
        except Exception as e:
            print(f"  âš  åŒèŠ±é¡º: {str(e)[:30]}")
            return []

    def search_stock_news(self, stock_name: str) -> List[Dict]:
        """æ ¹æ®è‚¡ç¥¨åç§°æœç´¢æ–°é—»ï¼ˆä»¥ç™¾åº¦ä¸ºä¾‹ï¼‰"""
        articles = []
        try:
            query = urllib.parse.quote(f"{stock_name} è‚¡ç¥¨ æ–°é—»")
            url = f"https://www.baidu.com/s?rtt=1&bsst=1&cl=2&tn=news&word={query}"
            r = requests.get(url, headers=self.headers, timeout=10)
            soup = BeautifulSoup(r.text, 'html.parser')
            for item in soup.select('.result-op.c-container.new-pmd')[:2]: # æ¯ä¸ªè‚¡ç¥¨å–2æ¡
                title_tag = item.select_one('h3 a')
                if title_tag:
                    articles.append({
                        'title': f"[{stock_name}] {title_tag.get_text(strip=True)}",
                        'content': item.select_one('.c-summary').get_text(strip=True) if item.select_one('.c-summary') else '',
                        'source': 'ç™¾åº¦æ–°é—»',
                        'category': 'stock_specific',
                        'url': title_tag['href'],
                        'published_at': datetime.now().isoformat()
                    })
            return articles
        except Exception as e:
            print(f"  âš  æœç´¢'{stock_name}'æ–°é—»å¤±è´¥: {e}")
            return []

    def scrape_all(self) -> List[Dict]:
        """çˆ¬å–æ‰€æœ‰å®˜æ–¹ç½‘ç«™"""
        all_articles = []
        success_count = 0
        scrapers = [
            ('ç¾è”å‚¨', self.scrape_fed),
            ('SEC', self.scrape_sec),
            ('å¤®è¡Œ', self.scrape_pbc),
            ('è¯ç›‘ä¼š', self.scrape_csrc),
            ('å¾®åšçƒ­æœ', self.scrape_weibo_hot_search),
            ('åŒèŠ±é¡º', self.scrape_tonghuashun),
        ]
        
        for name, scraper in scrapers:
            try:
                articles = scraper()
                all_articles.extend(articles)
                if articles:
                    print(f"  âœ“ {name}: {len(articles)}æ¡")
                    success_count += 1
                else:
                    print(f"  - {name}: 0æ¡")
            except Exception as e:
                print(f"  âš  {name}: {str(e)[:30]}")
        
        print(f"  ğŸ“Š çˆ¬è™«ç»Ÿè®¡: {success_count}/{len(scrapers)} æˆåŠŸ, å…± {len(all_articles)} æ¡")
        return all_articles
