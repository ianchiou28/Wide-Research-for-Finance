"""
æœˆåº¦æ·±åº¦åˆ†ææ¨¡å—
åŠŸèƒ½ï¼š
1. è‡ªåŠ¨è·å–/æŠ“å–æœˆåº¦é‡å¤§äº‹ä»¶ï¼ˆå¤®è¡Œä¼šè®®ã€ç»æµæ•°æ®å‘å¸ƒç­‰ï¼‰
2. æ·±åº¦åˆ†æäº‹ä»¶å¯¹å¸‚åœºçš„å½±å“
3. ç”ŸæˆåŠ å‡ä»“å»ºè®®
4. æ”¯æŒå¯¹è¯å¼äº¤äº’è¿½é—®
5. æ¯æ—¥æ›´æ–°ï¼ŒæŒç»­ä¿®æ­£é¢„æµ‹
"""

import os
import json
import re
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Tuple
from openai import OpenAI
import glob


class MonthlyAnalysis:
    """æœˆåº¦æ·±åº¦åˆ†æå™¨"""
    
    # é¢„è®¾çš„é‡å¤§äº‹ä»¶æ—¥å†ï¼ˆå¯æ‰©å±•ä¸ºè‡ªåŠ¨æŠ“å–ï¼‰
    MAJOR_EVENTS_TEMPLATE = {
        "us_fed": {
            "name": "ç¾è”å‚¨FOMCè®®æ¯ä¼šè®®",
            "name_en": "Fed FOMC Meeting",
            "frequency": "monthly",
            "importance": "critical",
            "affects": ["us_stocks", "global_stocks", "bonds", "forex", "crypto"],
            "keywords": ["FOMC", "ç¾è”å‚¨", "åˆ©ç‡å†³è®®", "é²å¨å°”", "Powell", "Fed"]
        },
        "us_cpi": {
            "name": "ç¾å›½CPIæ•°æ®",
            "name_en": "US CPI Data",
            "frequency": "monthly",
            "importance": "high",
            "affects": ["us_stocks", "bonds", "forex"],
            "keywords": ["CPI", "é€šèƒ€", "inflation", "ç‰©ä»·"]
        },
        "us_employment": {
            "name": "ç¾å›½éå†œå°±ä¸šæ•°æ®",
            "name_en": "US Non-Farm Payrolls",
            "frequency": "monthly",
            "importance": "high",
            "affects": ["us_stocks", "forex"],
            "keywords": ["éå†œ", "å°±ä¸š", "NFP", "employment", "payroll"]
        },
        "china_pboc": {
            "name": "ä¸­å›½äººæ°‘é“¶è¡Œåˆ©ç‡å†³è®®",
            "name_en": "PBOC Interest Rate Decision",
            "frequency": "monthly",
            "importance": "critical",
            "affects": ["cn_stocks", "hk_stocks", "forex"],
            "keywords": ["å¤®è¡Œ", "LPR", "é™æ¯", "é™å‡†", "è´§å¸æ”¿ç­–"]
        },
        "china_pmi": {
            "name": "ä¸­å›½PMIæ•°æ®",
            "name_en": "China PMI Data",
            "frequency": "monthly",
            "importance": "high",
            "affects": ["cn_stocks", "commodities"],
            "keywords": ["PMI", "åˆ¶é€ ä¸š", "é‡‡è´­ç»ç†"]
        },
        "china_cewc": {
            "name": "ä¸­å¤®ç»æµå·¥ä½œä¼šè®®",
            "name_en": "Central Economic Work Conference",
            "frequency": "yearly",  # æ¯å¹´12æœˆ
            "importance": "critical",
            "affects": ["cn_stocks", "hk_stocks", "commodities"],
            "keywords": ["ä¸­å¤®ç»æµå·¥ä½œä¼šè®®", "ç»æµå·¥ä½œä¼šè®®", "æ”¿ç­–å®šè°ƒ"]
        },
        "japan_boj": {
            "name": "æ—¥æœ¬å¤®è¡Œåˆ©ç‡å†³è®®",
            "name_en": "BOJ Interest Rate Decision",
            "frequency": "monthly",
            "importance": "high",
            "affects": ["jp_stocks", "forex", "us_stocks"],
            "keywords": ["æ—¥æœ¬å¤®è¡Œ", "BOJ", "æ—¥å…ƒ", "åŠ æ¯", "YCC"]
        },
        "ecb_meeting": {
            "name": "æ¬§æ´²å¤®è¡Œåˆ©ç‡å†³è®®",
            "name_en": "ECB Interest Rate Decision",
            "frequency": "monthly",
            "importance": "high",
            "affects": ["eu_stocks", "forex"],
            "keywords": ["æ¬§æ´²å¤®è¡Œ", "ECB", "æ¬§å…ƒ", "æ‹‰åŠ å¾·"]
        },
        "us_gdp": {
            "name": "ç¾å›½GDPæ•°æ®",
            "name_en": "US GDP Data",
            "frequency": "quarterly",
            "importance": "high",
            "affects": ["us_stocks", "global_stocks"],
            "keywords": ["GDP", "ç»æµå¢é•¿", "economic growth"]
        },
        "earnings_season": {
            "name": "è´¢æŠ¥å­£",
            "name_en": "Earnings Season",
            "frequency": "quarterly",
            "importance": "high",
            "affects": ["us_stocks", "cn_stocks"],
            "keywords": ["è´¢æŠ¥", "earnings", "ä¸šç»©", "å­£æŠ¥"]
        }
    }
    
    # é¢„è®¾äº‹ä»¶æ—¥å†ä½œä¸ºå¤‡ç”¨ï¼ˆè‡ªåŠ¨æŠ“å–å¤±è´¥æ—¶ä½¿ç”¨ï¼‰
    FALLBACK_CALENDAR = {
        "2025-01": [
            {"event_id": "us_employment", "date": "2025-01-10", "note": "12æœˆéå†œ"},
            {"event_id": "us_cpi", "date": "2025-01-15", "note": "12æœˆCPI"},
            {"event_id": "japan_boj", "date": "2025-01-24", "note": ""},
            {"event_id": "us_fed", "date": "2025-01-29", "note": "1æœˆFOMC"}
        ],
        "2025-02": [
            {"event_id": "us_employment", "date": "2025-02-07", "note": "1æœˆéå†œ"},
            {"event_id": "us_cpi", "date": "2025-02-12", "note": "1æœˆCPI"},
            {"event_id": "china_pmi", "date": "2025-02-01", "note": "1æœˆPMI"}
        ],
        "2025-03": [
            {"event_id": "us_employment", "date": "2025-03-07", "note": "2æœˆéå†œ"},
            {"event_id": "us_cpi", "date": "2025-03-12", "note": "2æœˆCPI"},
            {"event_id": "us_fed", "date": "2025-03-19", "note": "3æœˆFOMC"},
            {"event_id": "japan_boj", "date": "2025-03-14", "note": ""}
        ],
        "2025-12": [
            {"event_id": "us_employment", "date": "2025-12-05", "note": "11æœˆéå†œ"},
            {"event_id": "us_cpi", "date": "2025-12-11", "note": "11æœˆCPI"},
            {"event_id": "us_fed", "date": "2025-12-17", "note": "12æœˆFOMCï¼Œå¯èƒ½é™æ¯"},
            {"event_id": "japan_boj", "date": "2025-12-19", "note": "å…³æ³¨æ˜¯å¦åŠ æ¯"},
            {"event_id": "china_cewc", "date": "2025-12-11", "note": "ä¸­å¤®ç»æµå·¥ä½œä¼šè®®ï¼Œçº¦12æœˆä¸­æ—¬"},
            {"event_id": "ecb_meeting", "date": "2025-12-12", "note": ""}
        ]
    }
    
    # ç»æµæ—¥å†æ•°æ®æ¥æºé…ç½®
    CALENDAR_SOURCES = [
        {
            "name": "Investing.com Economic Calendar",
            "url": "https://www.investing.com/economic-calendar/",
            "type": "web_scrape"
        },
        {
            "name": "Trading Economics",
            "url": "https://tradingeconomics.com/calendar",
            "type": "web_scrape"  
        }
    ]
    
    def __init__(self):
        self.api_key = os.getenv('DEEPSEEK_API_KEY')
        self.client = None
        if self.api_key:
            self.client = OpenAI(api_key=self.api_key, base_url="https://api.deepseek.com")
        
        # å¯¹è¯å†å²ï¼ˆç”¨äºè¿½é—®ï¼‰
        self.conversation_history: List[Dict] = []
        self.current_analysis: Optional[Dict] = None
        
        # ç¼“å­˜çš„åŠ¨æ€äº‹ä»¶
        self._cached_events: Dict[str, List[Dict]] = {}
        self._cache_time: Optional[datetime] = None
        
    def get_month_key(self, date: datetime = None) -> str:
        """è·å–æœˆä»½é”®å€¼"""
        if date is None:
            date = datetime.now()
        return date.strftime("%Y-%m")
    
    def _fetch_events_from_news(self, year: int, month: int) -> List[Dict]:
        """é€šè¿‡åˆ†ææ–°é—»è‡ªåŠ¨è¯†åˆ«é‡å¤§äº‹ä»¶"""
        events = []
        
        # ä»å†å²æŠ¥å‘Šä¸­æå–ä¸äº‹ä»¶ç›¸å…³çš„æ–°é—»
        json_reports = glob.glob('data/reports_json/report_*.json')
        json_reports.sort(key=os.path.getctime, reverse=True)
        
        # æ”¶é›†æœ€è¿‘çš„æ–°é—»
        recent_news = []
        for report_path in json_reports[:30]:  # æœ€è¿‘30ä»½æŠ¥å‘Š
            try:
                with open(report_path, 'r', encoding='utf-8') as f:
                    report = json.load(f)
                
                for event_type in ['high_impact', 'other']:
                    for news in report.get('events', {}).get(event_type, []):
                        title = news.get('title', '')
                        summary = news.get('summary', '')
                        if title:
                            recent_news.append(f"{title}: {summary[:100]}")
            except:
                continue
        
        # å¦‚æœæ²¡æœ‰è¶³å¤Ÿçš„æ–°é—»æ•°æ®ï¼Œè·³è¿‡è‡ªåŠ¨è¯†åˆ«
        if len(recent_news) < 5 or not self.client:
            print("  æ–°é—»æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è‡ªåŠ¨è¯†åˆ«")
            return events
        
        # ç”¨AIåˆ†ææ–°é—»ä¸­æåˆ°çš„å³å°†å‘ç”Ÿçš„é‡å¤§äº‹ä»¶
        prompt = f"""åˆ†æä»¥ä¸‹æ–°é—»ï¼Œæå–{year}å¹´{month}æœˆå°†è¦å‘ç”Ÿçš„é‡å¤§ç»æµ/é‡‘èäº‹ä»¶ã€‚

ã€æœ€è¿‘æ–°é—»æ‘˜è¦ã€‘
{chr(10).join(recent_news[:30])}

è¯·æ‰¾å‡ºæ–°é—»ä¸­æ˜ç¡®æåˆ°çš„{year}å¹´{month}æœˆçš„äº‹ä»¶ï¼š
1. å¤®è¡Œä¼šè®®/åˆ©ç‡å†³è®®
2. é‡è¦ç»æµæ•°æ®å‘å¸ƒ
3. é‡å¤§æ”¿ç­–ä¼šè®®
4. å…¶ä»–å½±å“å¸‚åœºçš„äº‹ä»¶

åªè¿”å›æœ‰æ˜ç¡®æ—¥æœŸçš„äº‹ä»¶ï¼Œè¿”å›JSONæ•°ç»„ï¼š
[{{"date": "YYYY-MM-DD", "name": "äº‹ä»¶å", "importance": "critical/high/medium", "note": "è¯´æ˜"}}]

å¦‚æœæ²¡æ‰¾åˆ°ï¼Œè¿”å› []"""

        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=1500
            )
            
            content = response.choices[0].message.content if response.choices else None
            if content:
                # æå–JSONæ•°ç»„
                start = content.find('[')
                end = content.rfind(']')
                if start != -1 and end != -1 and end > start:
                    events = json.loads(content[start:end+1])
                    print(f"  è‡ªåŠ¨è¯†åˆ«åˆ° {len(events)} ä¸ªäº‹ä»¶")
        except Exception as e:
            print(f"  è‡ªåŠ¨æŠ“å–äº‹ä»¶å¤±è´¥: {e}")
        
        return events
    
    def _identify_high_impact_events(self, events: List[Dict]) -> List[Dict]:
        """ç”¨AIåˆ¤æ–­å“ªäº›äº‹ä»¶ä¼šå¯¹è‚¡å¸‚äº§ç”Ÿé‡å¤§å½±å“"""
        if not events or not self.client or len(events) < 2:
            return events
        
        events_desc = "\n".join([f"- {e.get('date', '')}: {e.get('name', '')} ({e.get('importance', 'medium')})" for e in events[:15]])
        
        prompt = f"""è¯„ä¼°ä»¥ä¸‹äº‹ä»¶å¯¹è‚¡å¸‚çš„å½±å“ï¼š

{events_desc}

å¯¹æ¯ä¸ªäº‹ä»¶è¯„ä¼°å½±å“ç¨‹åº¦(1-10)å’Œæ–¹å‘ã€‚è¿”å›JSONæ•°ç»„ï¼š
[{{"date": "æ—¥æœŸ", "name": "åç§°", "impact_score": 8, "expected_direction": "bullish/bearish/neutral", "analysis": "ç®€çŸ­åˆ†æ"}}]"""

        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                max_tokens=1500
            )
            
            content = response.choices[0].message.content if response.choices else None
            if content:
                start = content.find('[')
                end = content.rfind(']')
                if start != -1 and end != -1:
                    impact_analysis = json.loads(content[start:end+1])
                    
                    # åˆå¹¶å½±å“åˆ†æåˆ°äº‹ä»¶ä¸­
                    impact_map = {(a.get('date', ''), a.get('name', '')): a for a in impact_analysis}
                    for event in events:
                        key = (event.get('date', ''), event.get('name', ''))
                        if key in impact_map:
                            event.update(impact_map[key])
                    
                    # æŒ‰å½±å“åˆ†æ•°æ’åº
                    events.sort(key=lambda x: x.get('impact_score', 0), reverse=True)
                    print(f"  å·²è¯„ä¼° {len(impact_analysis)} ä¸ªäº‹ä»¶çš„å½±å“")
        except Exception as e:
            print(f"  å½±å“è¯„ä¼°å¤±è´¥: {e}")
        
        return events
    
    def _fix_json_with_ai(self, broken_json: str, events: List[Dict]) -> Optional[Dict]:
        """ä½¿ç”¨AIä¿®å¤æ ¼å¼é”™è¯¯çš„JSON"""
        if not self.client:
            return None
        
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªJSONä¿®å¤ä¸“å®¶ã€‚ç”¨æˆ·ä¼šç»™ä½ ä¸€ä¸ªæ ¼å¼æœ‰é—®é¢˜çš„JSONå­—ç¬¦ä¸²ï¼Œè¯·ä¿®å¤å®ƒå¹¶è¿”å›æœ‰æ•ˆçš„JSONã€‚åªè¿”å›ä¿®å¤åçš„JSONï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—ã€‚"},
                    {"role": "user", "content": f"è¯·ä¿®å¤è¿™ä¸ªJSON:\n{broken_json[:4000]}"}
                ],
                temperature=0.1,
                max_tokens=6000
            )
            
            content = response.choices[0].message.content if response.choices else None
            if content:
                content = content.strip()
                if content.startswith('```json'):
                    content = content[7:]
                elif content.startswith('```'):
                    content = content[3:]
                if content.endswith('```'):
                    content = content[:-3]
                
                start = content.find('{')
                end = content.rfind('}')
                if start != -1 and end != -1:
                    return json.loads(content[start:end+1])
        except Exception as e:
            print(f"AIä¿®å¤JSONå¤±è´¥: {e}")
        
        return None
    
    def get_monthly_events(self, year: int = None, month: int = None, force_refresh: bool = False) -> List[Dict]:
        """è·å–æŒ‡å®šæœˆä»½çš„é‡å¤§äº‹ä»¶ - æ”¯æŒè‡ªåŠ¨æŠ“å–å’ŒåŠ¨æ€æ›´æ–°"""
        if year is None:
            year = datetime.now().year
        if month is None:
            month = datetime.now().month
        
        month_key = f"{year}-{month:02d}"
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆ24å°æ—¶å†…ï¼‰
        cache_valid = (
            not force_refresh and 
            self._cache_time and 
            datetime.now() - self._cache_time < timedelta(hours=24) and
            month_key in self._cached_events
        )
        
        if cache_valid:
            return self._cached_events[month_key]
        
        # 1. é¦–å…ˆå°è¯•ä»æ–°é—»ä¸­è‡ªåŠ¨è¯†åˆ«äº‹ä»¶
        print(f"æ­£åœ¨è‡ªåŠ¨è¯†åˆ« {year}å¹´{month}æœˆ çš„é‡å¤§äº‹ä»¶...")
        auto_events = self._fetch_events_from_news(year, month)
        
        # 2. è·å–é¢„è®¾çš„äº‹ä»¶æ—¥å†
        preset_events = self.FALLBACK_CALENDAR.get(month_key, [])
        preset_enriched = []
        for event in preset_events:
            event_info = self.MAJOR_EVENTS_TEMPLATE.get(event["event_id"], {})
            preset_enriched.append({
                "id": event["event_id"],
                "date": event["date"],
                "name": event_info.get("name", event["event_id"]),
                "name_en": event_info.get("name_en", ""),
                "importance": event_info.get("importance", "medium"),
                "affects": event_info.get("affects", []),
                "note": event.get("note", ""),
                "keywords": event_info.get("keywords", []),
                "source": "preset"
            })
        
        # 3. åˆå¹¶è‡ªåŠ¨æŠ“å–å’Œé¢„è®¾äº‹ä»¶ï¼ˆå»é‡ï¼Œä½†å…è®¸åŒä¸€å¤©å¤šä¸ªäº‹ä»¶ï¼‰
        all_events = []
        seen_keys = set()
        
        # ä¼˜å…ˆä½¿ç”¨è‡ªåŠ¨æŠ“å–çš„äº‹ä»¶
        for event in auto_events:
            key = (event.get('date', ''), event.get('name', ''))
            if key not in seen_keys:
                event['source'] = 'auto_detected'
                all_events.append(event)
                seen_keys.add(key)
        
        # è¡¥å……é¢„è®¾äº‹ä»¶ï¼ˆå…è®¸åŒä¸€å¤©æœ‰å¤šä¸ªä¸åŒäº‹ä»¶ï¼‰
        for event in preset_enriched:
            key = (event.get('date', ''), event.get('name', ''))
            if key not in seen_keys:
                all_events.append(event)
                seen_keys.add(key)
        
        # 4. è¯„ä¼°äº‹ä»¶å½±å“ç¨‹åº¦å¹¶æ’åº
        if all_events and self.client:
            print("æ­£åœ¨è¯„ä¼°äº‹ä»¶å¯¹å¸‚åœºçš„å½±å“...")
            all_events = self._identify_high_impact_events(all_events)
        
        # æŒ‰æ—¥æœŸæ’åº
        all_events.sort(key=lambda x: x.get("date", ""))
        
        # æ›´æ–°ç¼“å­˜
        self._cached_events[month_key] = all_events
        self._cache_time = datetime.now()
        
        return all_events
    
    def collect_related_news(self, events: List[Dict], days_back: int = 30) -> Dict[str, List[Dict]]:
        """ä»å†å²æŠ¥å‘Šä¸­æ”¶é›†ä¸äº‹ä»¶ç›¸å…³çš„æ–°é—»"""
        # ä½¿ç”¨äº‹ä»¶åç§°ä½œä¸ºkeyï¼ˆå› ä¸ºè‡ªåŠ¨æŠ“å–çš„äº‹ä»¶å¯èƒ½æ²¡æœ‰idï¼‰
        news_by_event = {}
        for e in events:
            key = e.get("id") or e.get("name", "")
            news_by_event[key] = []
        
        # è·å–è¿‡å»Nå¤©çš„JSONæŠ¥å‘Š
        json_reports = glob.glob('data/reports_json/report_*.json')
        cutoff = datetime.now() - timedelta(days=days_back)
        
        for report_path in json_reports:
            try:
                mtime = datetime.fromtimestamp(os.path.getctime(report_path))
                if mtime < cutoff:
                    continue
                
                with open(report_path, 'r', encoding='utf-8') as f:
                    report = json.load(f)
                
                # æ£€æŸ¥æ¯ä¸ªäº‹ä»¶çš„å…³é”®è¯
                all_news = []
                for event_type in ['high_impact', 'other', 'stock_specific']:
                    all_news.extend(report.get('events', {}).get(event_type, []))
                
                for news in all_news:
                    title = news.get('title', '') + ' ' + news.get('summary', '')
                    title_lower = title.lower()
                    
                    for event in events:
                        keywords = event.get('keywords', [])
                        # å¯¹äºè‡ªåŠ¨æŠ“å–çš„äº‹ä»¶ï¼Œç”¨åç§°ä½œä¸ºå…³é”®è¯
                        if not keywords:
                            keywords = [event.get('name', ''), event.get('name_en', '')]
                        
                        for keyword in keywords:
                            if keyword and keyword.lower() in title_lower:
                                key = event.get("id") or event.get("name", "")
                                if key in news_by_event:
                                    news_by_event[key].append({
                                        "title": news.get("title", ""),
                                        "summary": news.get("summary", ""),
                                        "sentiment": news.get("sentiment", {}),
                                        "date": report.get("meta", {}).get("generated_at", "")
                                    })
                                break
            except Exception as e:
                continue
        
        return news_by_event
    
    def aggregate_weekly_data(self, days: int = 30) -> Dict:
        """èšåˆè¿‡å»Nå¤©çš„å‘¨æŠ¥æ•°æ®"""
        weekly_files = glob.glob('data/weekly/analysis_*.json')
        weekly_files.sort(key=os.path.getctime, reverse=True)
        
        cutoff = datetime.now() - timedelta(days=days)
        
        all_stocks = {}
        summaries = []
        
        for filepath in weekly_files[:8]:  # æœ€è¿‘8ä»½å‘¨æŠ¥ï¼ˆçº¦2ä¸ªæœˆï¼‰
            try:
                mtime = datetime.fromtimestamp(os.path.getctime(filepath))
                if mtime < cutoff:
                    continue
                    
                with open(filepath, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                summaries.append(data.get('summary', ''))
                
                for stock in data.get('stocks', []):
                    symbol = stock.get('symbol', '')
                    if symbol not in all_stocks:
                        all_stocks[symbol] = {
                            'name': stock.get('name', ''),
                            'predictions': [],
                            'reasons': []
                        }
                    all_stocks[symbol]['predictions'].append(stock.get('prediction', ''))
                    all_stocks[symbol]['reasons'].append(stock.get('reason', ''))
            except:
                continue
        
        return {
            'stocks': all_stocks,
            'summaries': summaries
        }
    
    def generate_monthly_analysis(self, year: int = None, month: int = None) -> Dict:
        """ç”Ÿæˆæœˆåº¦æ·±åº¦åˆ†æï¼ˆæ”¯æŒæ¯æ—¥æ›´æ–°ï¼‰"""
        if year is None:
            year = datetime.now().year
        if month is None:
            month = datetime.now().month
        
        # 1. è·å–æœ¬æœˆé‡å¤§äº‹ä»¶
        events = self.get_monthly_events(year, month)
        
        # 2. æ ‡è®°å·²å‘ç”Ÿçš„äº‹ä»¶ï¼Œæ”¶é›†å®é™…ç»“æœ
        today = datetime.now().strftime('%Y-%m-%d')
        past_events = []
        upcoming_events = []
        for e in events:
            event_date = e.get('date', '9999-12-31')
            if event_date <= today:
                e['status'] = 'completed'
                past_events.append(e)
            else:
                e['status'] = 'upcoming'
                upcoming_events.append(e)
        
        # 3. æ”¶é›†ç›¸å…³æ–°é—»ï¼ˆç”¨äºåˆ†æå·²å‘ç”Ÿäº‹ä»¶çš„å®é™…å½±å“ï¼‰
        related_news = self.collect_related_news(events)
        
        # 4. èšåˆå†å²å‘¨æŠ¥æ•°æ®
        weekly_data = self.aggregate_weekly_data()
        
        # 5. è·å–ä¸Šä¸€æ¬¡åˆ†æï¼ˆç”¨äºå¯¹æ¯”å’Œä¿®æ­£ï¼‰
        previous_analysis = self.get_latest_analysis()
        
        # 6. æ„å»ºåˆ†ææç¤º
        month_name = f"{year}å¹´{month}æœˆ"
        current_date = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')
        
        # å·²å‘ç”Ÿäº‹ä»¶åŠå…¶å®é™…å½±å“
        past_events_desc = ""
        if past_events:
            past_events_desc = "\nã€å·²å‘ç”Ÿäº‹ä»¶å›é¡¾ã€‘"
            for e in past_events:
                key = e.get("id") or e.get("name", "")
                news = related_news.get(key, [])
                past_events_desc += f"\n- {e.get('date')} {e.get('name')} [å·²å‘ç”Ÿ]"
                if news:
                    # åˆ†æå®é™…å¸‚åœºååº”
                    sentiments = [n.get('sentiment', {}).get('label', '') for n in news[-5:]]
                    past_events_desc += f"\n  ç›¸å…³æ–°é—»{len(news)}æ¡ï¼Œè¿‘æœŸæƒ…ç»ª: {', '.join(set(sentiments))}"
        
        # å³å°†å‘ç”Ÿçš„äº‹ä»¶
        upcoming_events_desc = ""
        if upcoming_events:
            upcoming_events_desc = "\nã€å³å°†å‘ç”Ÿäº‹ä»¶ã€‘"
            for e in upcoming_events:
                key = e.get("id") or e.get("name", "")
                news_count = len(related_news.get(key, []))
                importance = e.get('importance', 'medium')
                source_tag = "[è‡ªåŠ¨è¯†åˆ«]" if e.get('source') == 'auto_detected' else ""
                
                upcoming_events_desc += f"\n- {e.get('date', 'å¾…å®š')} {e.get('name', '')}ï¼ˆé‡è¦æ€§ï¼š{importance}ï¼‰{source_tag}"
                
                if e.get('impact_score'):
                    upcoming_events_desc += f" [å½±å“è¯„åˆ†: {e['impact_score']}/10]"
                if e.get('expected_direction'):
                    direction_map = {'bullish': 'åˆ©å¤š', 'bearish': 'åˆ©ç©º', 'neutral': 'ä¸­æ€§'}
                    upcoming_events_desc += f" [é¢„æœŸ: {direction_map.get(e['expected_direction'], e['expected_direction'])}]"
                
                if e.get('note'):
                    upcoming_events_desc += f" - {e['note']}"
                upcoming_events_desc += f" [ç›¸å…³æ–°é—»{news_count}æ¡]"
        
        events_desc = past_events_desc + upcoming_events_desc
        
        # æ±‡æ€»å‘¨æŠ¥ä¸­çš„é«˜é¢‘è‚¡ç¥¨
        top_stocks = sorted(
            weekly_data['stocks'].items(),
            key=lambda x: len(x[1]['predictions']),
            reverse=True
        )[:15]
        
        stocks_desc = ""
        for symbol, data in top_stocks:
            pred_counts = {}
            for p in data['predictions']:
                pred_counts[p] = pred_counts.get(p, 0) + 1
            main_pred = max(pred_counts.items(), key=lambda x: x[1])[0] if pred_counts else "æœªçŸ¥"
            stocks_desc += f"\n- {symbol} ({data['name']}): ä¸»è¦é¢„æµ‹={main_pred}, å‡ºç°{len(data['predictions'])}æ¬¡"
        
        # æœ€è¿‘çš„å¸‚åœºæ€»ç»“
        recent_summaries = "\n".join(weekly_data['summaries'][:3]) if weekly_data['summaries'] else "æš‚æ— è¿‘æœŸå‘¨æŠ¥"
        
        # ä¸Šæ¬¡é¢„æµ‹å›é¡¾ï¼ˆç”¨äºä¿®æ­£ï¼‰
        previous_summary = ""
        if previous_analysis and not previous_analysis.get('error'):
            prev_date = previous_analysis.get('generated_at', '')[:10]
            prev_summary = previous_analysis.get('summary', '')[:200]
            previous_summary = f"\nã€ä¸Šæ¬¡åˆ†æå›é¡¾ã€‘({prev_date})\n{prev_summary}..."
        
        prompt = f"""ä½ æ˜¯ä¸€ä½èµ„æ·±é‡‘èåˆ†æå¸ˆã€‚ä»Šå¤©æ˜¯{current_date}ï¼Œè¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œç”Ÿæˆ{month_name}çš„æ·±åº¦æœˆåº¦å¸‚åœºåˆ†ææŠ¥å‘Šã€‚

ã€é‡è¦ã€‘è¿™æ˜¯æ¯æ—¥æ›´æ–°çš„æœˆåº¦åˆ†æï¼Œè¯·ç‰¹åˆ«æ³¨æ„ï¼š
1. å¯¹å·²å‘ç”Ÿäº‹ä»¶ï¼Œåˆ†æå…¶å®é™…å¸‚åœºå½±å“ï¼Œä¿®æ­£ä¹‹å‰çš„é¢„æµ‹
2. å¯¹å³å°†å‘ç”Ÿäº‹ä»¶ï¼Œç»“åˆæœ€æ–°ä¿¡æ¯æ›´æ–°é¢„æœŸ
3. æ ¹æ®å¸‚åœºå˜åŒ–åŠæ—¶è°ƒæ•´åŠ å‡ä»“å»ºè®®

{events_desc}
{previous_summary}

ã€è¿‘æœŸå‘¨æŠ¥å¸‚åœºè§‚ç‚¹ã€‘
{recent_summaries}

ã€è¿‘æœŸé«˜å…³æ³¨åº¦è‚¡ç¥¨ã€‘
{stocks_desc}

è¯·æä¾›è¯¦å°½çš„åˆ†ææŠ¥å‘Šï¼Œå¿…é¡»åŒ…å«ï¼š

1. **æœ¬æœˆå®è§‚ç¯å¢ƒæ¦‚è§ˆ**ï¼ˆç»“åˆä»Šæ—¥æœ€æ–°æƒ…å†µï¼‰
   - å…¨çƒç»æµå½¢åŠ¿
   - ä¸»è¦å¤®è¡Œæ”¿ç­–é¢„æœŸ
   - åœ°ç¼˜æ”¿æ²»é£é™©

2. **é‡å¤§äº‹ä»¶æ·±åº¦åˆ†æ**
   å·²å‘ç”Ÿäº‹ä»¶ï¼š
   - å®é™…ç»“æœä¸é¢„æœŸå¯¹æ¯”
   - å¸‚åœºååº”åˆ†æ
   - åç»­å½±å“è¯„ä¼°
   
   å³å°†å‘ç”Ÿäº‹ä»¶ï¼š
   - æœ€æ–°å¸‚åœºé¢„æœŸ
   - æƒ…æ™¯åˆ†æï¼ˆä¹è§‚/åŸºå‡†/æ‚²è§‚ï¼‰
   - å¯¹å„ç±»èµ„äº§çš„å½±å“é¢„åˆ¤

3. **è¡Œä¸šè½®åŠ¨å»ºè®®**ï¼ˆæ ¹æ®æœ€æ–°æƒ…å†µè°ƒæ•´ï¼‰
   - æœ¬æœˆçœ‹å¥½çš„è¡Œä¸šåŠåŸå› 
   - æœ¬æœˆéœ€å›é¿çš„è¡Œä¸šåŠåŸå› 
   - è¾¹é™…å˜åŒ–å€¼å¾—å…³æ³¨çš„è¡Œä¸š

4. **ä¸ªè‚¡åŠ å‡ä»“å»ºè®®**ï¼ˆä»Šæ—¥å»ºè®®ï¼‰
   ç»™å‡ºå…·ä½“çš„æ“ä½œå»ºè®®ï¼š
   - å»ºè®®åŠ ä»“çš„è‚¡ç¥¨ï¼ˆé™„ç†ç”±å’Œç›®æ ‡ä½ï¼‰
   - å»ºè®®å‡ä»“çš„è‚¡ç¥¨ï¼ˆé™„ç†ç”±å’Œæ­¢æŸä½ï¼‰
   - è§‚æœ›çš„è‚¡ç¥¨ï¼ˆéœ€è¦ç­‰å¾…çš„ä¿¡å·ï¼‰

5. **å…³é”®æ—¶é—´èŠ‚ç‚¹æé†’**ï¼ˆæœªæ¥å¾…å‘ç”Ÿçš„ï¼‰
   - é‡ç‚¹å…³æ³¨æ—¥æœŸ
   - éœ€è¦æå‰å¸ƒå±€çš„æ—¶æœº
   - é£é™©é‡Šæ”¾çš„å¯èƒ½æ—¶é—´çª—å£

6. **é¢„æµ‹ä¿®æ­£**ï¼ˆå¦‚æœ‰ä¸Šæ¬¡åˆ†æï¼‰
   - å“ªäº›é¢„æµ‹å‡†ç¡®/åå·®
   - éœ€è¦è°ƒæ•´çš„è§‚ç‚¹
   - æ–°çš„é£é™©ç‚¹

7. **é£é™©æç¤º**
   - ä¸»è¦ä¸ç¡®å®šæ€§
   - é»‘å¤©é¹…äº‹ä»¶é¢„è­¦
   - ä»“ä½ç®¡ç†å»ºè®®

è¯·è¿”å›JSONæ ¼å¼ï¼ˆä»…è¿”å›JSONï¼Œæ— å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
  "month": "{month_name}",
  "update_date": "{current_date}",
  "generated_at": "{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
  "is_daily_update": true,
  "macro_overview": {{
    "global_economy": "å…¨çƒç»æµæ¦‚è¿°",
    "central_banks": "å¤®è¡Œæ”¿ç­–é¢„æœŸ",
    "geopolitics": "åœ°ç¼˜æ”¿æ²»é£é™©"
  }},
  "event_analysis": [
    {{
      "event": "äº‹ä»¶åç§°",
      "date": "æ—¥æœŸ",
      "status": "completed/upcoming",
      "market_expectation": "é¢„æœŸæˆ–å®é™…ç»“æœ",
      "actual_result": "å·²å‘ç”Ÿäº‹ä»¶çš„å®é™…ç»“æœï¼ˆå¦‚é€‚ç”¨ï¼‰",
      "scenarios": {{
        "bullish": "ä¹è§‚æƒ…æ™¯åŠæ¦‚ç‡",
        "base": "åŸºå‡†æƒ…æ™¯åŠæ¦‚ç‡",
        "bearish": "æ‚²è§‚æƒ…æ™¯åŠæ¦‚ç‡"
      }},
      "impact": {{
        "stocks": "å¯¹è‚¡å¸‚å½±å“",
        "bonds": "å¯¹å€ºå¸‚å½±å“",
        "forex": "å¯¹æ±‡å¸‚å½±å“",
        "commodities": "å¯¹å¤§å®—å•†å“å½±å“"
      }},
      "key_indicators": ["éœ€å…³æ³¨çš„æŒ‡æ ‡1", "æŒ‡æ ‡2"]
    }}
  ],
  "prediction_review": {{
    "accurate": ["å‡†ç¡®çš„é¢„æµ‹1"],
    "adjusted": ["éœ€è°ƒæ•´çš„è§‚ç‚¹1"],
    "new_risks": ["æ–°å‘ç°çš„é£é™©1"]
  }},
  "sector_rotation": {{
    "overweight": [
      {{"sector": "è¡Œä¸šå", "reason": "çœ‹å¥½åŸå› ", "top_picks": ["ä»£è¡¨è‚¡1", "ä»£è¡¨è‚¡2"]}}
    ],
    "underweight": [
      {{"sector": "è¡Œä¸šå", "reason": "å›é¿åŸå› "}}
    ],
    "watch": [
      {{"sector": "è¡Œä¸šå", "catalyst": "éœ€å…³æ³¨çš„å‚¬åŒ–å‰‚"}}
    ]
  }},
  "stock_recommendations": {{
    "buy": [
      {{
        "symbol": "è‚¡ç¥¨ä»£ç ",
        "name": "è‚¡ç¥¨åç§°",
        "current_price": "å½“å‰ä»·æ ¼åŒºé—´",
        "target_price": "ç›®æ ‡ä»·",
        "stop_loss": "æ­¢æŸä½",
        "reason": "æ¨èç†ç”±",
        "timing": "å»ºä»“æ—¶æœº"
      }}
    ],
    "sell": [
      {{
        "symbol": "è‚¡ç¥¨ä»£ç ",
        "name": "è‚¡ç¥¨åç§°",
        "reason": "å‡ä»“ç†ç”±",
        "timing": "å‡ä»“æ—¶æœº"
      }}
    ],
    "hold": [
      {{
        "symbol": "è‚¡ç¥¨ä»£ç ",
        "name": "è‚¡ç¥¨åç§°",
        "wait_for": "ç­‰å¾…çš„ä¿¡å·"
      }}
    ]
  }},
  "key_dates": [
    {{
      "date": "æ—¥æœŸ",
      "event": "äº‹ä»¶",
      "action": "å»ºè®®æ“ä½œ",
      "priority": "high/medium/low"
    }}
  ],
  "risk_warnings": {{
    "main_uncertainties": ["ä¸ç¡®å®šæ€§1", "ä¸ç¡®å®šæ€§2"],
    "black_swan_alerts": ["æ½œåœ¨é»‘å¤©é¹…1"],
    "position_management": "ä»“ä½ç®¡ç†å»ºè®®"
  }},
  "summary": "æœˆåº¦æ€»ç»“ï¼ˆä¸€æ®µè¯æ¦‚æ‹¬ï¼‰"
}}"""
        
        if not self.client:
            return {
                "error": True,
                "message": "æœªé…ç½® DeepSeek API",
                "events": events
            }
        
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±é‡‘èåˆ†æå¸ˆï¼Œæ“…é•¿å®è§‚åˆ†æå’ŒæŠ•èµ„ç­–ç•¥åˆ¶å®šã€‚è¯·æä¾›ä¸“ä¸šã€å®¢è§‚ã€å¯æ“ä½œçš„åˆ†æå»ºè®®ã€‚è¯·ä¸¥æ ¼è¿”å›æœ‰æ•ˆçš„JSONæ ¼å¼ï¼Œä¸è¦åŒ…å«ä»»ä½•æ³¨é‡Šæˆ–é¢å¤–æ–‡å­—ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=8000
            )
            
            content = response.choices[0].message.content if response.choices else None
            if content:
                # æ¸…ç†å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
                content = content.strip()
                if content.startswith('```json'):
                    content = content[7:]
                elif content.startswith('```'):
                    content = content[3:]
                if content.endswith('```'):
                    content = content[:-3]
                content = content.strip()
                
                # æå–JSON
                start = content.find('{')
                end = content.rfind('}')
                if start != -1 and end != -1 and end > start:
                    json_str = content[start:end+1]
                    
                    # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
                    try:
                        analysis = json.loads(json_str)
                    except json.JSONDecodeError as je:
                        # å°è¯•ä¿®å¤ï¼šç§»é™¤å°¾éƒ¨é€—å·
                        fixed_json = re.sub(r',\s*([}\]])', r'\1', json_str)
                        try:
                            analysis = json.loads(fixed_json)
                        except:
                            # å¦‚æœä»ç„¶å¤±è´¥ï¼Œç”¨AIä¿®å¤
                            print(f"JSONè§£æå¤±è´¥ï¼Œå°è¯•AIä¿®å¤...")
                            analysis = self._fix_json_with_ai(json_str, events)
                            if not analysis:
                                return {
                                    "error": True,
                                    "message": f"JSONè§£æå¤±è´¥: {str(je)}",
                                    "events": events,
                                    "raw_content": json_str[:1000]
                                }
                    
                    # ä¿å­˜åˆ†æç»“æœ
                    self.current_analysis = analysis
                    
                    # åˆå§‹åŒ–å¯¹è¯å†å²
                    self.conversation_history = [
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±é‡‘èåˆ†æå¸ˆã€‚ä½ åˆšåˆšç”Ÿæˆäº†æœˆåº¦åˆ†ææŠ¥å‘Šï¼Œç”¨æˆ·å¯èƒ½ä¼šè¿½é—®ç»†èŠ‚ã€‚è¯·åŸºäºå·²æœ‰åˆ†æå†…å®¹å›ç­”é—®é¢˜ï¼Œå¦‚éœ€è¡¥å……å¯ä»¥æä¾›æ›´æ·±å…¥çš„è§è§£ã€‚"},
                        {"role": "assistant", "content": f"æˆ‘å·²å®Œæˆ{month_name}çš„æœˆåº¦åˆ†ææŠ¥å‘Šã€‚æ‚¨å¯ä»¥å°±ä»»ä½•æ„Ÿå…´è¶£çš„éƒ¨åˆ†è¿½é—®ï¼Œæ¯”å¦‚ï¼š\n- æŸä¸ªå…·ä½“äº‹ä»¶çš„æ›´è¯¦ç»†åˆ†æ\n- æŸåªè‚¡ç¥¨çš„æ·±åº¦ç ”ç©¶\n- ç‰¹å®šè¡Œä¸šçš„æŠ•èµ„é€»è¾‘\n- ä»“ä½é…ç½®çš„å…·ä½“å»ºè®®"}
                    ]
                    
                    return analysis
                    
        except Exception as e:
            return {
                "error": True,
                "message": f"ç”Ÿæˆåˆ†æå¤±è´¥: {str(e)}",
                "events": events
            }
        
        return {
            "error": True,
            "message": "AIè¿”å›å†…å®¹è§£æå¤±è´¥",
            "events": events
        }
    
    def chat(self, user_message: str) -> str:
        """å¯¹è¯å¼è¿½é—®"""
        if not self.client:
            return "æœªé…ç½® DeepSeek APIï¼Œæ— æ³•è¿›è¡Œå¯¹è¯"
        
        if not self.current_analysis:
            return "è¯·å…ˆç”Ÿæˆæœˆåº¦åˆ†ææŠ¥å‘Šï¼Œå†è¿›è¡Œè¿½é—®"
        
        # æ·»åŠ å½“å‰åˆ†æçš„ä¸Šä¸‹æ–‡
        context = f"ã€å½“å‰æœˆåº¦åˆ†ææŠ¥å‘Šæ‘˜è¦ã€‘\n{json.dumps(self.current_analysis, ensure_ascii=False, indent=2)[:3000]}..."
        
        # æ„å»ºæ¶ˆæ¯
        messages = self.conversation_history.copy()
        messages.append({
            "role": "user", 
            "content": f"{context}\n\nç”¨æˆ·é—®é¢˜: {user_message}"
        })
        
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=messages,
                temperature=0.4,
                max_tokens=2000
            )
            
            assistant_reply = response.choices[0].message.content if response.choices else "æŠ±æ­‰ï¼Œæ— æ³•ç”Ÿæˆå›å¤"
            
            # æ›´æ–°å¯¹è¯å†å²
            self.conversation_history.append({"role": "user", "content": user_message})
            self.conversation_history.append({"role": "assistant", "content": assistant_reply})
            
            # ä¿æŒå†å²é•¿åº¦
            if len(self.conversation_history) > 20:
                self.conversation_history = self.conversation_history[:2] + self.conversation_history[-16:]
            
            return assistant_reply
            
        except Exception as e:
            return f"å¯¹è¯å‡ºé”™: {str(e)}"
    
    def save_analysis(self, analysis: Dict) -> str:
        """ä¿å­˜åˆ†æç»“æœ"""
        if not analysis or analysis.get('error'):
            return ""
        
        output_dir = os.path.join('data', 'monthly')
        os.makedirs(output_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = os.path.join(output_dir, f'analysis_{timestamp}.json')
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        return filename
    
    def get_latest_analysis(self) -> Optional[Dict]:
        """è·å–æœ€æ–°çš„æœˆåº¦åˆ†æ"""
        files = glob.glob('data/monthly/analysis_*.json')
        if not files:
            return None
        
        latest = max(files, key=os.path.getctime)
        try:
            with open(latest, 'r', encoding='utf-8') as f:
                return json.load(f)
        except:
            return None
    
    def update_event_result(self, event_id: str, actual_result: str, market_reaction: str) -> Dict:
        """æ›´æ–°äº‹ä»¶ç»“æœï¼Œç”¨äºå›æµ‹å’Œä¿®æ­£é¢„æµ‹"""
        if not self.current_analysis:
            return {"error": "æ— å½“å‰åˆ†æ"}
        
        update_prompt = f"""åŸºäºä»¥ä¸‹å®é™…ç»“æœï¼Œè¯·æ›´æ–°å’Œä¿®æ­£ä¹‹å‰çš„é¢„æµ‹ï¼š

ã€åŸäº‹ä»¶é¢„æµ‹ã€‘
äº‹ä»¶ID: {event_id}

ã€å®é™…ç»“æœã€‘
{actual_result}

ã€å¸‚åœºååº”ã€‘
{market_reaction}

è¯·æä¾›ï¼š
1. é¢„æµ‹å‡†ç¡®åº¦è¯„ä¼°
2. åå·®åŸå› åˆ†æ
3. åç»­å½±å“ä¿®æ­£
4. æ–°çš„æŠ•èµ„å»ºè®®è°ƒæ•´

è¿”å›JSONæ ¼å¼ã€‚"""
        
        if not self.client:
            return {"error": "æœªé…ç½®API"}
        
        try:
            response = self.client.chat.completions.create(
                model="deepseek-chat",
                messages=[{"role": "user", "content": update_prompt}],
                temperature=0.3,
                max_tokens=1500
            )
            
            content = response.choices[0].message.content if response.choices else None
            if content:
                start = content.find('{')
                end = content.rfind('}')
                if start != -1 and end != -1:
                    return json.loads(content[start:end+1])
        except Exception as e:
            return {"error": str(e)}
        
        return {"error": "è§£æå¤±è´¥"}


# ä¾¿æ·å‡½æ•°
def generate_monthly_report(year: int = None, month: int = None) -> Dict:
    """ç”Ÿæˆæœˆåº¦æŠ¥å‘Šçš„ä¾¿æ·å‡½æ•°"""
    analyzer = MonthlyAnalysis()
    analysis = analyzer.generate_monthly_analysis(year, month)
    if not analysis.get('error'):
        analyzer.save_analysis(analysis)
    return analysis


if __name__ == '__main__':
    # æµ‹è¯•
    analyzer = MonthlyAnalysis()
    
    # è·å–12æœˆäº‹ä»¶ï¼ˆæ”¯æŒè‡ªåŠ¨æŠ“å–ï¼‰
    print("=" * 60)
    print("æ­£åœ¨è·å–2025å¹´12æœˆé‡å¤§äº‹ä»¶ï¼ˆè‡ªåŠ¨è¯†åˆ« + é¢„è®¾ï¼‰...")
    print("=" * 60)
    
    events = analyzer.get_monthly_events(2025, 12)
    
    print(f"\nå…±å‘ç° {len(events)} ä¸ªé‡å¤§äº‹ä»¶ï¼š\n")
    for e in events:
        source_tag = "[è‡ªåŠ¨]" if e.get('source') == 'auto_detected' else "[é¢„è®¾]"
        impact = f"å½±å“:{e.get('impact_score', '-')}/10" if e.get('impact_score') else ""
        direction = e.get('expected_direction', '')
        
        print(f"  ğŸ“… {e.get('date', 'å¾…å®š')} - {e.get('name', '')} ({e.get('importance', 'medium')}) {source_tag}")
        if impact or direction:
            print(f"     {impact} {direction}")
        if e.get('analysis'):
            print(f"     ğŸ’¡ {e['analysis'][:80]}...")
    
    print("\n" + "=" * 60)
    print("æ­£åœ¨ç”Ÿæˆæœˆåº¦æ·±åº¦åˆ†æ...")
    print("=" * 60)
    
    analysis = analyzer.generate_monthly_analysis(2025, 12)
    
    if not analysis.get('error'):
        filename = analyzer.save_analysis(analysis)
        print(f"\nâœ… åˆ†æå·²ä¿å­˜: {filename}")
        print(f"\nğŸ“ æœˆåº¦æ€»ç»“:")
        print("-" * 40)
        print(analysis.get('summary', 'æ— æ€»ç»“'))
        
        # æ˜¾ç¤ºå…³é”®å»ºè®®
        if analysis.get('stock_recommendations'):
            recs = analysis['stock_recommendations']
            if recs.get('buy'):
                print(f"\nğŸ“ˆ å»ºè®®åŠ ä»“ ({len(recs['buy'])}åª):")
                for stock in recs['buy'][:3]:
                    print(f"   - {stock.get('symbol', '')} {stock.get('name', '')}: {stock.get('reason', '')[:50]}")
            if recs.get('sell'):
                print(f"\nğŸ“‰ å»ºè®®å‡ä»“ ({len(recs['sell'])}åª):")
                for stock in recs['sell'][:3]:
                    print(f"   - {stock.get('symbol', '')} {stock.get('name', '')}: {stock.get('reason', '')[:50]}")
    else:
        print(f"\nâŒ é”™è¯¯: {analysis.get('message')}")
        if analysis.get('raw_content'):
            print(f"\nåŸå§‹å†…å®¹é¢„è§ˆ: {analysis['raw_content'][:500]}...")
